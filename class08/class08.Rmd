---
title: 'Class 8: Machine Learning'
author: "Saeed Ghassemzadeh"
date: "4/25/2019"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-means clustering

Let's start with an example of running the **kmeans()** function. 

```{r}
# Generate some example data for clustering

# rnorm gets a random number from a normal distribution
# we asked for 30 points centered around -3, and 30 points centered around +3
tmp <- c(rnorm(30,-3), rnorm(30,3)) 

# plotting it?
x <- cbind(x=tmp, y=rev(tmp))
plot(x)
```


Use the kmeans() function setting k to 2 and nstart=20

```{r}
km <- kmeans(x, centers=2, nstart=20)
```


Inspect/print the results
> Q. How many points are in each cluster? ----> 30 (found when you print just km)

> Q. What ‘component’ of your result object details

>
>      - cluster size?

```{r}
km$size
```

>
>      - cluster assignment/membership?

```{r}
km$cluster  
```

>
>      - cluster center?

```{r}
km$centers
```
      
> Plot x colored by the kmeans cluster assignment and
      add cluster centers as blue points

```{r}
km <- kmeans(x, centers=2, nstart=20)
plot(x, col = km$cluster)
points(km$centers, pch=18, col="blue", cex=3)
```



## Hierarchical Clustering example

We must give the **hclust()** function a distance matrix, not the raw data as input

```{r}
# Distance matrix calculation
d <- dist(x)

# Clustering 
hc <- hclust(d)

plot(hc)
```


```{r}
plot(hc)
abline(h=6, col="red")
cutree(hc, h=6) #cut by height h
cutree(hc, k=2) #should return the same answer as above
# and it does!
```





Another example but more real life like example


```{r}
 # Step 1. Generate some example data for clustering
x <- rbind(
  matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2),   # c1
  matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
  matrix(c(rnorm(50, mean = 1, sd = 0.3),           # c3
           rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
# Step 2. Plot the data without clustering
plot(x)
# Step 3. Generate colors for known clusters
#         (just so we can compare to hclust results)
col <- as.factor( rep(c("c1","c2","c3"), each=50) )
plot(x, col=col)
```

> Q. Use the dist(), hclust(), plot() and cutree() functions to return 2 and 3 clusters
> Q. How does this compare to your known 'col' groups?


```{r}
hc <- hclust(dist(x))
plot(hc)
abline(h=2, col="red")
abline(h=2.8, col="blue")
```


```{r}
gp2 <- cutree(hc, k=2)
gp3 <- cutree(hc, k=3)
```


```{r}
gp2
```

```{r}
gp3
```


```{r}
plot(x, col=gp2)
```


```{r}
plot(x, col=gp3)
```


```{r}
table(gp2, gp3)
```




------------------------------------------------------------------------------



# Principal Component Analysis

We will use the **prcomp()** function for PCA
```{r}
# You can also download this file from the class website!
mydata <- read.csv("https://tinyurl.com/expression-CSV", row.names=1)
```


```{r}
head(mydata) # by default head() just prints the first 6; head(mydata, 10) will print the first 10
```


```{r}
nrow(mydata) # how many genes we have
```

```{r}
ncol(mydata) # how many samples we have for each gene
```

```{r}
colnames(mydata) # names of columns/samples
```


```{r}
head(t(mydata)) # transpose to make genes the columns, samples the rows
# prcomp() requires it this way
```


Running our PCA analysis on the transpose of our data
```{r}
pca <- prcomp(t(mydata), scale=TRUE)
```


PCA plot
```{r}
# A basic PC1 vs PC2 2-D plot
plot(pca$x[,1], pca$x[,2], xlab = "PC1", ylab = "PC2")
```


Calculate the percent variance captures in each PC
```{r}
## Precent variance is often more informative to look at
pca.var <- pca$sdev^2
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)

head(pca.var.per) #printing the first few, we see that PC1 is most important, and all subsequent are less important
```


```{r}
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
# confirming that PC1 is the most important and what we need
```


```{r}
## A vector of colors for wt and ko samples
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"
plot(pca$x[,1], pca$x[,2], col=colvec, pch=16, xlab=paste0("PC1 (", pca.var.per[1], "%)"), ylab=paste0("PC2 (", pca.var.per[2], "%)"))
   
```





--------------------------------------------------------------------------------------------------------------

## UK Food
https://bioboot.github.io/bimm143_S19/class-material/lab-8-bimm143.html


```{r}
x <- read.csv("UK_foods.csv")
```

```{r}
## Complete the following code to find out how many rows and columns are in x?
nrow(x)
ncol(x)
dim(x)
```


```{r}
head(x)
# we actually have 4 columns, not 5. the error comes from the first column (of row labels) being included as a real column. fix is below
```



```{r}
# Note how the minus indexing works
rownames(x) <- x[,1]
x <- x[,-1]
head(x) # now the column of row labels isn't counted as a real column. 
```


```{r}
# check the dimensions; columns is 4 now
dim(x)
```


> Side-note: An alternative approach to setting the correct row-names in this case would be to read the data filie again and this time set the row.names argument of read.csv() to be the first column (i.e. use argument setting row.names=1), see below:
x <- read.csv("data/UK_foods.csv", row.names=1)
head(x)


```{r}
x <- read.csv("UK_foods.csv", row.names=1)
head(x)
```


```{r}
dim(x) # still works 
```


```{r}
barplot(as.matrix(x), beside=F, col=rainbow(nrow(x)))
# unclear graphs when beside=T; beside=F leads to stacked bars (and is default)
```



```{r}
pairs(x, col=rainbow(10), pch=16)
```



```{r}
# Use the prcomp() PCA function 
# Need to transpose because prcomp() expects observations (countries) in rows, variables (foods) in columns
pca <- prcomp(t(x))
summary(pca)
```



```{r}
# Plot PC1 vs PC2
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col = c("orange", "red", "blue", "dark green"))
```



```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```


```{r}
## or the second row here...
z <- summary(pca)
z$importance
```



```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


```{r}
## Lets focus on PC1 as it accounts for > 90% of variance 
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
# positive values push countries to the right, negative values push countries to the left
```



```{r}
## now focus on PC2 instead
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```



























